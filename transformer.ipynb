{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b96f886",
   "metadata": {},
   "source": [
    "Implenntation of Transformer Arch \n",
    "\n",
    "<img src=\"image.png\" width=\"500\" height=\"600\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d549f606",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import argparse\n",
    "import os\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from tensorboardX import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "from dataset import problem\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79588b4",
   "metadata": {},
   "source": [
    "UTILS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99740ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "\n",
    "class utils:\n",
    "    @staticmethod\n",
    "    def get_loss(pred, ans, vocab_size, label_smoothing, pad):\n",
    "        # took this \"normalizing\" from tensor2tensor. We subtract it for\n",
    "        # readability. This makes no difference on learning.\n",
    "        confidence = 1.0 - label_smoothing\n",
    "        low_confidence = (1.0 - confidence) / float(vocab_size - 1)\n",
    "        normalizing = -(\n",
    "            confidence * math.log(confidence) + float(vocab_size - 1) *\n",
    "            low_confidence * math.log(low_confidence + 1e-20))\n",
    "        one_hot = torch.zeros_like(pred).scatter_(1, ans.unsqueeze(1), 1)\n",
    "        one_hot = one_hot * confidence + (1 - one_hot) * low_confidence\n",
    "        log_prob = F.log_softmax(pred, dim=1)\n",
    "        xent = -(one_hot * log_prob).sum(dim=1)\n",
    "        xent = xent.masked_select(ans != pad)\n",
    "        loss = (xent - normalizing).mean()\n",
    "        return loss\n",
    "\n",
    "    @staticmethod\n",
    "    def get_accuracy(pred, ans, pad):\n",
    "        pred = pred.max(1)[1]\n",
    "        n_correct = pred.eq(ans)\n",
    "        n_correct = n_correct.masked_select(ans != pad)\n",
    "        return n_correct.sum().item() / n_correct.size(0)\n",
    "\n",
    "    @staticmethod\n",
    "    def save_checkpoint(model, filepath, global_step, is_best):\n",
    "        model_save_path = filepath + '/last_model.pt'\n",
    "        torch.save(model, model_save_path)\n",
    "        torch.save(global_step, filepath + '/global_step.pt')\n",
    "        if is_best:\n",
    "            best_save_path = filepath + '/best_model.pt'\n",
    "            shutil.copyfile(model_save_path, best_save_path)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_checkpoint(model_path, device, is_eval=True):\n",
    "        if is_eval:\n",
    "            model = torch.load(model_path + '/best_model.pt')\n",
    "            model.eval()\n",
    "            return model.to(device=device)\n",
    "        model = torch.load(model_path + '/last_model.pt')\n",
    "        global_step = torch.load(model_path + '/global_step.pt')\n",
    "        return model.to(device=device), global_step\n",
    "\n",
    "    @staticmethod\n",
    "    def create_pad_mask(t, pad):\n",
    "        mask = (t == pad).unsqueeze(-2)\n",
    "        return mask\n",
    "\n",
    "    @staticmethod\n",
    "    def create_trg_self_mask(target_len, device=None):\n",
    "        # Prevent leftward information flow in self-attention.\n",
    "        ones = torch.ones(target_len, target_len, dtype=torch.uint8,\n",
    "                          device=device)\n",
    "        t_self_mask = torch.triu(ones, diagonal=1).unsqueeze(0)\n",
    "        return t_self_mask\n",
    "\n",
    "def forward(self, inputs, targets):\n",
    "    enc_output, i_mask = None, None\n",
    "    if self.has_inputs:\n",
    "        i_mask = utils.create_pad_mask(inputs, self.src_pad_idx)\n",
    "        enc_output = self.encode(inputs, i_mask)\n",
    "    \n",
    "    t_mask = utils.create_pad_mask(targets, self.trg_pad_idx)\n",
    "    target_size = targets.size()[1]\n",
    "    t_self_mask = utils.create_trg_self_mask(target_size,\n",
    "                                             device=targets.device)\n",
    "    return self.decode(targets, enc_output, i_mask, t_self_mask, t_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59f46b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LRScheduler:\n",
    "    def __init__(self, parameters, hidden_size, warmup, step=0):\n",
    "        self.constant = 2.0 * (hidden_size ** -0.5)\n",
    "        self.cur_step = step\n",
    "        self.warmup = warmup\n",
    "        self.optimizer = optim.Adam(parameters, lr=self.learning_rate(),\n",
    "                                    betas=(0.9, 0.997), eps=1e-09)\n",
    "\n",
    "    def step(self):\n",
    "        self.cur_step += 1\n",
    "        rate = self.learning_rate()\n",
    "        for p in self.optimizer.param_groups:\n",
    "            p['lr'] = rate\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def zero_grad(self):\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "    def learning_rate(self):\n",
    "        lr = self.constant\n",
    "        lr *= min(1.0, self.cur_step / self.warmup)\n",
    "        lr *= max(self.cur_step, self.warmup) ** -0.5\n",
    "        return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7cc9e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weight(x):\n",
    "    nn.init.xavier_uniform_(x.weight)\n",
    "    if x.bias is not None:\n",
    "        nn.init.constant_(x.bias, 0)\n",
    "        \n",
    "        \n",
    "class FeedForwardNetwork(nn.Module):\n",
    "     def __init__(self, hidden_size, filter_size, dropout_rate):\n",
    "        super(FeedForwardNetwork, self).__init__()\n",
    "\n",
    "        self.layer1 = nn.Linear(hidden_size, filter_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.layer2 = nn.Linear(filter_size, hidden_size)\n",
    "\n",
    "        initialize_weight(self.layer1)\n",
    "        initialize_weight(self.layer2)\n",
    "        \n",
    "     def forward(self,x):\n",
    "        x= self.layer1(x)\n",
    "        x= self.relu(x)\n",
    "        x= self.dropout(x)\n",
    "        x= self.layer(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e77705d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, dropout_rate, head_size=8):\n",
    "        super(MultiHeadAttention,self).__init__()\n",
    "        \n",
    "        self.head_size = head_size\n",
    "        self.att_size = att_size = hidden_size // head_size\n",
    "        self.scale = att_size ** -0.5\n",
    "        \n",
    "        self.linear_q = nn.Linear(hidden_size,head_size*att_size, bias = False)\n",
    "        self.linear_k = nn.Linear(hidden_size,head_size*att_size, bias = False)\n",
    "        self.linear_v = nn.Linear(hidden_size,head_size*att_size, bias = False)\n",
    "        \n",
    "        initialize_weight(self.linear_q)\n",
    "        initialize_weight(self.linear_k)\n",
    "        initialize_weight(self.linear_v)\n",
    "        \n",
    "        self.att_dropout = nn.Dropout(dropout_rate)\n",
    "        self.output_layer = nn.Linear(head_size*att_size,hidden_size,bias = False)\n",
    "        \n",
    "        initialize_weight(self.output_layer)\n",
    "    \n",
    "    def forward(self, q, k, v, mask, cache=None):\n",
    "        orig_q_size = q.size()\n",
    "\n",
    "        d_k = self.att_size\n",
    "        d_v = self.att_size\n",
    "        batch_size = q.size(0)\n",
    "\n",
    "        # head_i = Attention(Q(W^Q)_i, K(W^K)_i, V(W^V)_i)\n",
    "        q = self.linear_q(q).view(batch_size, -1, self.head_size, d_k)\n",
    "        if cache is not None and 'encdec_k' in cache:\n",
    "            k, v = cache['encdec_k'], cache['encdec_v']\n",
    "        else:\n",
    "            k = self.linear_k(k).view(batch_size, -1, self.head_size, d_k)\n",
    "            v = self.linear_v(v).view(batch_size, -1, self.head_size, d_v)\n",
    "\n",
    "            if cache is not None:\n",
    "                cache['encdec_k'], cache['encdec_v'] = k, v\n",
    "\n",
    "        q = q.transpose(1, 2)                  # [b, h, q_len, d_k]\n",
    "        v = v.transpose(1, 2)                  # [b, h, v_len, d_v]\n",
    "        k = k.transpose(1, 2).transpose(2, 3)  # [b, h, d_k, k_len]\n",
    "\n",
    "        # Scaled Dot-Product Attention.\n",
    "        # Attention(Q, K, V) = softmax((QK^T)/sqrt(d_k))V\n",
    "        q.mul_(self.scale)\n",
    "        x = torch.matmul(q, k)  # [b, h, q_len, k_len]\n",
    "        x.masked_fill_(mask.unsqueeze(1), -1e9)\n",
    "        x = torch.softmax(x, dim=3)\n",
    "        x = self.att_dropout(x)\n",
    "        x = x.matmul(v)  # [b, h, q_len, attn]\n",
    "\n",
    "        x = x.transpose(1, 2).contiguous()  # [b, q_len, h, attn]\n",
    "        x = x.view(batch_size, -1, self.head_size * d_v)\n",
    "\n",
    "        x = self.output_layer(x)\n",
    "\n",
    "        assert x.size() == orig_q_size\n",
    "        return x\n",
    "\n",
    "        \n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1bfc218",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self,hidden_size,filter_size,dropout_rate):\n",
    "        super(EncoderLayer,self).__init__()\n",
    "        \n",
    "        self.self_attention_norm = nn.LayerNorm(hidden_size, eps=1e-6)\n",
    "        self.self_attention = MultiHeadAttention(hidden_size, dropout_rate)\n",
    "        self.self_attention_dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.ffn_norm = nn.LayerNorm(hidden_size, eps=1e-6)\n",
    "        self.ffn = FeedForwardNetwork(hidden_size, filter_size, dropout_rate)\n",
    "        self.ffn_dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x, mask):  \n",
    "        y = self.self_attention_norm(x)\n",
    "        y = self.self_attention(y, y, y, mask)\n",
    "        y = self.self_attention_dropout(y)\n",
    "        x = x + y\n",
    "\n",
    "        y = self.ffn_norm(x)\n",
    "        y = self.ffn(y)\n",
    "        y = self.ffn_dropout(y)\n",
    "        x = x + y\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43b78193",
   "metadata": {},
   "outputs": [],
   "source": [
    "class  DecoderLayer(nn.Module):\n",
    "    def __init__(self, hidden_size, filter_size, dropout_rate):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.self_attention_norm = nn.LayerNorm(hidden_size, eps=1e-6)\n",
    "        self.self_attention = MultiHeadAttention(hidden_size, dropout_rate)\n",
    "        self.self_attention_dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.enc_dec_attention_norm = nn.LayerNorm(hidden_size, eps=1e-6)\n",
    "        self.enc_dec_attention = MultiHeadAttention(hidden_size, dropout_rate)\n",
    "        self.enc_dec_attention_dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.ffn_norm = nn.LayerNorm(hidden_size, eps=1e-6)\n",
    "        self.ffn = FeedForwardNetwork(hidden_size, filter_size, dropout_rate)\n",
    "        self.ffn_dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x, enc_output, self_mask, i_mask, cache):\n",
    "        y = self.self_attention_norm(x)\n",
    "        y = self.self_attention(y, y, y, self_mask)\n",
    "        y = self.self_attention_dropout(y)\n",
    "        x = x + y\n",
    "\n",
    "        if enc_output is not None:\n",
    "            y = self.enc_dec_attention_norm(x)\n",
    "            y = self.enc_dec_attention(y, enc_output, enc_output, i_mask,\n",
    "                                       cache)\n",
    "            y = self.enc_dec_attention_dropout(y)\n",
    "            x = x + y\n",
    "\n",
    "        y = self.ffn_norm(x)\n",
    "        y = self.ffn(y)\n",
    "        y = self.ffn_dropout(y)\n",
    "        x = x + y\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7abd2bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, hidden_size, filter_size, dropout_rate, n_layers):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        encoders = [EncoderLayer(hidden_size, filter_size, dropout_rate)\n",
    "                    for _ in range(n_layers)]\n",
    "        self.layers = nn.ModuleList(encoders)\n",
    "\n",
    "        self.last_norm = nn.LayerNorm(hidden_size, eps=1e-6)\n",
    "\n",
    "    def forward(self, inputs, mask):\n",
    "        encoder_output = inputs\n",
    "        for enc_layer in self.layers:\n",
    "            encoder_output = enc_layer(encoder_output, mask)\n",
    "        return self.last_norm(encoder_output)\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hidden_size, filter_size, dropout_rate, n_layers):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        decoders = [DecoderLayer(hidden_size, filter_size, dropout_rate)\n",
    "                    for _ in range(n_layers)]\n",
    "        self.layers = nn.ModuleList(decoders)\n",
    "\n",
    "        self.last_norm = nn.LayerNorm(hidden_size, eps=1e-6)\n",
    "\n",
    "    def forward(self, targets, enc_output, i_mask, t_self_mask, cache):\n",
    "        decoder_output = targets\n",
    "        for i, dec_layer in enumerate(self.layers):\n",
    "            layer_cache = None\n",
    "            if cache is not None:\n",
    "                if i not in cache:\n",
    "                    cache[i] = {}\n",
    "                layer_cache = cache[i]\n",
    "            decoder_output = dec_layer(decoder_output, enc_output,\n",
    "                                       t_self_mask, i_mask, layer_cache)\n",
    "        return self.last_norm(decoder_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "26cd8e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, i_vocab_size, t_vocab_size,\n",
    "                 n_layers=6,\n",
    "                 hidden_size=512,\n",
    "                 filter_size=2048,\n",
    "                 dropout_rate=0.1,\n",
    "                 share_target_embedding=True,\n",
    "                 has_inputs=True,\n",
    "                 src_pad_idx=None,\n",
    "                 trg_pad_idx=None):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.emb_scale = hidden_size ** 0.5\n",
    "        self.has_inputs = has_inputs\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "\n",
    "        self.t_vocab_embedding = nn.Embedding(t_vocab_size, hidden_size)\n",
    "        nn.init.normal_(self.t_vocab_embedding.weight, mean=0,\n",
    "                        std=hidden_size**-0.5)\n",
    "        self.t_emb_dropout = nn.Dropout(dropout_rate)\n",
    "        self.decoder = Decoder(hidden_size, filter_size,\n",
    "                               dropout_rate, n_layers)\n",
    "\n",
    "        if has_inputs:\n",
    "            if not share_target_embedding:\n",
    "                self.i_vocab_embedding = nn.Embedding(i_vocab_size,\n",
    "                                                      hidden_size)\n",
    "                nn.init.normal_(self.i_vocab_embedding.weight, mean=0,\n",
    "                                std=hidden_size**-0.5)\n",
    "            else:\n",
    "                self.i_vocab_embedding = self.t_vocab_embedding\n",
    "\n",
    "            self.i_emb_dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "            self.encoder = Encoder(hidden_size, filter_size,\n",
    "                                   dropout_rate, n_layers)\n",
    "\n",
    "        # For positional encoding\n",
    "        num_timescales = self.hidden_size // 2\n",
    "        max_timescale = 10000.0\n",
    "        min_timescale = 1.0\n",
    "        log_timescale_increment = (\n",
    "            math.log(float(max_timescale) / float(min_timescale)) /\n",
    "            max(num_timescales - 1, 1))\n",
    "        inv_timescales = min_timescale * torch.exp(\n",
    "            torch.arange(num_timescales, dtype=torch.float32) *\n",
    "            -log_timescale_increment)\n",
    "        self.register_buffer('inv_timescales', inv_timescales)\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        enc_output, i_mask = None, None\n",
    "        if self.has_inputs:\n",
    "            i_mask = utils.create_pad_mask(inputs, self.src_pad_idx)\n",
    "            enc_output = self.encode(inputs, i_mask)\n",
    "\n",
    "        t_mask = utils.create_pad_mask(targets, self.trg_pad_idx)\n",
    "        target_size = targets.size()[1]\n",
    "        t_self_mask = utils.create_trg_self_mask(target_size,\n",
    "                                                 device=targets.device)\n",
    "        return self.decode(targets, enc_output, i_mask, t_self_mask, t_mask)\n",
    "\n",
    "    def encode(self, inputs, i_mask):\n",
    "        # Input embedding\n",
    "        input_embedded = self.i_vocab_embedding(inputs)\n",
    "        input_embedded.masked_fill_(i_mask.squeeze(1).unsqueeze(-1), 0)\n",
    "        input_embedded *= self.emb_scale\n",
    "        input_embedded += self.get_position_encoding(inputs)\n",
    "        input_embedded = self.i_emb_dropout(input_embedded)\n",
    "\n",
    "        return self.encoder(input_embedded, i_mask)\n",
    "\n",
    "    def decode(self, targets, enc_output, i_mask, t_self_mask, t_mask,\n",
    "               cache=None):\n",
    "        # target embedding\n",
    "        target_embedded = self.t_vocab_embedding(targets)\n",
    "        target_embedded.masked_fill_(t_mask.squeeze(1).unsqueeze(-1), 0)\n",
    "\n",
    "        # Shifting\n",
    "        target_embedded = target_embedded[:, :-1]\n",
    "        target_embedded = F.pad(target_embedded, (0, 0, 1, 0))\n",
    "\n",
    "        target_embedded *= self.emb_scale\n",
    "        target_embedded += self.get_position_encoding(targets)\n",
    "        target_embedded = self.t_emb_dropout(target_embedded)\n",
    "\n",
    "        # decoder\n",
    "        decoder_output = self.decoder(target_embedded, enc_output, i_mask,\n",
    "                                      t_self_mask, cache)\n",
    "        # linear\n",
    "        output = torch.matmul(decoder_output,\n",
    "                              self.t_vocab_embedding.weight.transpose(0, 1))\n",
    "\n",
    "        return output\n",
    "\n",
    "    def get_position_encoding(self, x):\n",
    "        max_length = x.size()[1]\n",
    "        position = torch.arange(max_length, dtype=torch.float32,\n",
    "                                device=x.device)\n",
    "        scaled_time = position.unsqueeze(1) * self.inv_timescales.unsqueeze(0)\n",
    "        signal = torch.cat([torch.sin(scaled_time), torch.cos(scaled_time)],\n",
    "                           dim=1)\n",
    "        signal = F.pad(signal, (0, 0, 0, self.hidden_size % 2))\n",
    "        signal = signal.view(1, max_length, self.hidden_size)\n",
    "        return signal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2b435e",
   "metadata": {},
   "source": [
    "TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d294c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_train(writer, global_step, last_time, model, opt,\n",
    "                    inputs, targets, optimizer, loss, pred, ans):\n",
    "    if opt.summary_grad:\n",
    "        for name, param in model.named_parameters():\n",
    "            if not param.requires_grad:\n",
    "                continue\n",
    "\n",
    "            norm = torch.norm(param.grad.data.view(-1))\n",
    "            writer.add_scalar('gradient_norm/' + name, norm,\n",
    "                              global_step)\n",
    "\n",
    "    writer.add_scalar('input_stats/batch_size',\n",
    "                      targets.size(0), global_step)\n",
    "\n",
    "    if inputs is not None:\n",
    "        writer.add_scalar('input_stats/input_length',\n",
    "                          inputs.size(1), global_step)\n",
    "        i_nonpad = (inputs != opt.src_pad_idx).view(-1).type(torch.float32)\n",
    "        writer.add_scalar('input_stats/inputs_nonpadding_frac',\n",
    "                          i_nonpad.mean(), global_step)\n",
    "\n",
    "    writer.add_scalar('input_stats/target_length',\n",
    "                      targets.size(1), global_step)\n",
    "    t_nonpad = (targets != opt.trg_pad_idx).view(-1).type(torch.float32)\n",
    "    writer.add_scalar('input_stats/target_nonpadding_frac',\n",
    "                      t_nonpad.mean(), global_step)\n",
    "\n",
    "    writer.add_scalar('optimizer/learning_rate',\n",
    "                      optimizer.learning_rate(), global_step)\n",
    "\n",
    "    writer.add_scalar('loss', loss.item(), global_step)\n",
    "\n",
    "    acc = utils.get_accuracy(pred, ans, opt.trg_pad_idx)\n",
    "    writer.add_scalar('training/accuracy',\n",
    "                      acc, global_step)\n",
    "\n",
    "    steps_per_sec = 100.0 / (time.time() - last_time)\n",
    "    writer.add_scalar('global_step/sec', steps_per_sec,\n",
    "                      global_step)\n",
    "\n",
    "\n",
    "def train(train_data, model, opt, global_step, optimizer, t_vocab_size,\n",
    "          label_smoothing, writer):\n",
    "    model.train()\n",
    "    last_time = time.time()\n",
    "    pbar = tqdm(total=len(train_data.dataset), ascii=True)\n",
    "    for batch in train_data:\n",
    "        inputs = None\n",
    "        if opt.has_inputs:\n",
    "            inputs = batch.src\n",
    "\n",
    "        targets = batch.trg\n",
    "        pred = model(inputs, targets)\n",
    "\n",
    "        pred = pred.view(-1, pred.size(-1))\n",
    "        ans = targets.view(-1)\n",
    "\n",
    "        loss = utils.get_loss(pred, ans, t_vocab_size,\n",
    "                              label_smoothing, opt.trg_pad_idx)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if global_step % 100 == 0:\n",
    "            summarize_train(writer, global_step, last_time, model, opt,\n",
    "                            inputs, targets, optimizer, loss, pred, ans)\n",
    "            last_time = time.time()\n",
    "\n",
    "        pbar.set_description('[Loss: {:.4f}]'.format(loss.item()))\n",
    "\n",
    "        global_step += 1\n",
    "        pbar.update(targets.size(0))\n",
    "\n",
    "    pbar.close()\n",
    "    train_data.reload_examples()\n",
    "    return global_step\n",
    "\n",
    "\n",
    "def validation(validation_data, model, global_step, t_vocab_size, val_writer,\n",
    "               opt):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_cnt = 0\n",
    "    for batch in validation_data:\n",
    "        inputs = None\n",
    "        if opt.has_inputs:\n",
    "            inputs = batch.src\n",
    "        targets = batch.trg\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pred = model(inputs, targets)\n",
    "\n",
    "            pred = pred.view(-1, pred.size(-1))\n",
    "            ans = targets.view(-1)\n",
    "            loss = utils.get_loss(pred, ans, t_vocab_size, 0,\n",
    "                                  opt.trg_pad_idx)\n",
    "        total_loss += loss.item() * len(batch)\n",
    "        total_cnt += len(batch)\n",
    "\n",
    "    val_loss = total_loss / total_cnt\n",
    "    print(\"Validation Loss\", val_loss)\n",
    "    val_writer.add_scalar('loss', val_loss, global_step)\n",
    "    return val_loss\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--problem', required=True)\n",
    "    parser.add_argument('--train_step', type=int, default=200)\n",
    "    parser.add_argument('--batch_size', type=int, default=4096)\n",
    "    parser.add_argument('--max_length', type=int, default=100)\n",
    "    parser.add_argument('--n_layers', type=int, default=6)\n",
    "    parser.add_argument('--hidden_size', type=int, default=512)\n",
    "    parser.add_argument('--filter_size', type=int, default=2048)\n",
    "    parser.add_argument('--warmup', type=int, default=16000)\n",
    "    parser.add_argument('--val_every', type=int, default=5)\n",
    "    parser.add_argument('--dropout', type=float, default=0.1)\n",
    "    parser.add_argument('--label_smoothing', type=float, default=0.1)\n",
    "    parser.add_argument('--model', type=str, default='transformer')\n",
    "    parser.add_argument('--output_dir', type=str, default='./output')\n",
    "    parser.add_argument('--data_dir', type=str, default='./data')\n",
    "    parser.add_argument('--no_cuda', action='store_true')\n",
    "    parser.add_argument('--parallel', action='store_true')\n",
    "    parser.add_argument('--summary_grad', action='store_true')\n",
    "    opt = parser.parse_args()\n",
    "\n",
    "    device = torch.device('cpu' if opt.no_cuda else 'cuda')\n",
    "\n",
    "    if not os.path.exists(opt.output_dir + '/last/models'):\n",
    "        os.makedirs(opt.output_dir + '/last/models')\n",
    "    if not os.path.exists(opt.data_dir):\n",
    "        os.makedirs(opt.data_dir)\n",
    "\n",
    "    train_data, validation_data, i_vocab_size, t_vocab_size, opt = \\\n",
    "        problem.prepare(opt.problem, opt.data_dir, opt.max_length,\n",
    "                        opt.batch_size, device, opt)\n",
    "    if i_vocab_size is not None:\n",
    "        print(\"# of vocabs (input):\", i_vocab_size)\n",
    "    print(\"# of vocabs (target):\", t_vocab_size)\n",
    "\n",
    "    if opt.model == 'transformer':\n",
    "        from model.transformer import Transformer\n",
    "        model_fn = Transformer\n",
    "    elif opt.model == 'fast_transformer':\n",
    "        from model.fast_transformer import FastTransformer\n",
    "        model_fn = FastTransformer\n",
    "\n",
    "    if os.path.exists(opt.output_dir + '/last/models/last_model.pt'):\n",
    "        print(\"Load a checkpoint...\")\n",
    "        last_model_path = opt.output_dir + '/last/models'\n",
    "        model, global_step = utils.load_checkpoint(last_model_path, device,\n",
    "                                                   is_eval=False)\n",
    "    else:\n",
    "        model = model_fn(i_vocab_size, t_vocab_size,\n",
    "                         n_layers=opt.n_layers,\n",
    "                         hidden_size=opt.hidden_size,\n",
    "                         filter_size=opt.filter_size,\n",
    "                         dropout_rate=opt.dropout,\n",
    "                         share_target_embedding=opt.share_target_embedding,\n",
    "                         has_inputs=opt.has_inputs,\n",
    "                         src_pad_idx=opt.src_pad_idx,\n",
    "                         trg_pad_idx=opt.trg_pad_idx)\n",
    "        model = model.to(device=device)\n",
    "        global_step = 0\n",
    "\n",
    "    if opt.parallel:\n",
    "        print(\"Use\", torch.cuda.device_count(), \"GPUs\")\n",
    "        model = torch.nn.DataParallel(model)\n",
    "\n",
    "    num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(\"# of parameters: {}\".format(num_params))\n",
    "\n",
    "    optimizer = LRScheduler(\n",
    "        filter(lambda x: x.requires_grad, model.parameters()),\n",
    "        opt.hidden_size, opt.warmup, step=global_step)\n",
    "\n",
    "    writer = SummaryWriter(opt.output_dir + '/last')\n",
    "    val_writer = SummaryWriter(opt.output_dir + '/last/val')\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    for t_step in range(opt.train_step):\n",
    "        print(\"Epoch\", t_step)\n",
    "        start_epoch_time = time.time()\n",
    "        global_step = train(train_data, model, opt, global_step,\n",
    "                            optimizer, t_vocab_size, opt.label_smoothing,\n",
    "                            writer)\n",
    "        print(\"Epoch Time: {:.2f} sec\".format(time.time() - start_epoch_time))\n",
    "\n",
    "        if t_step % opt.val_every != 0:\n",
    "            continue\n",
    "\n",
    "        val_loss = validation(validation_data, model, global_step,\n",
    "                              t_vocab_size, val_writer, opt)\n",
    "        utils.save_checkpoint(model, opt.output_dir + '/last/models',\n",
    "                              global_step, val_loss < best_val_loss)\n",
    "        best_val_loss = min(val_loss, best_val_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fdf283d",
   "metadata": {},
   "source": [
    "Decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21ee189",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] --data_dir DATA_DIR --model_dir MODEL_DIR\n",
      "                             [--max_length MAX_LENGTH] [--beam_size BEAM_SIZE]\n",
      "                             [--alpha ALPHA] [--no_cuda] [--translate]\n",
      "ipykernel_launcher.py: error: the following arguments are required: --data_dir, --model_dir\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "# import argparse\n",
    "# import time\n",
    "\n",
    "# import torch\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "# # pylint: disable=not-callable\n",
    "\n",
    "\n",
    "# def encode_inputs(sentence, model, src_data, beam_size, device):\n",
    "#     inputs = src_data['field'].preprocess(sentence)\n",
    "#     inputs.append(src_data['field'].eos_token)\n",
    "#     inputs = [inputs]\n",
    "#     inputs = src_data['field'].process(inputs, device=device)\n",
    "#     with torch.no_grad():\n",
    "#         src_mask = utils.create_pad_mask(inputs, src_data['pad_idx'])\n",
    "#         enc_output = model.encode(inputs, src_mask)\n",
    "#         enc_output = enc_output.repeat(beam_size, 1, 1)\n",
    "#     return enc_output, src_mask\n",
    "\n",
    "\n",
    "# def update_targets(targets, best_indices, idx, vocab_size):\n",
    "#     best_tensor_indices = torch.div(best_indices, vocab_size)\n",
    "#     best_token_indices = torch.fmod(best_indices, vocab_size)\n",
    "#     new_batch = torch.index_select(targets, 0, best_tensor_indices)\n",
    "#     new_batch[:, idx] = best_token_indices\n",
    "#     return new_batch\n",
    "\n",
    "\n",
    "# def get_result_sentence(indices_history, trg_data, vocab_size):\n",
    "#     result = []\n",
    "#     k = 0\n",
    "#     for best_indices in indices_history[::-1]:\n",
    "#         best_idx = best_indices[k]\n",
    "#         # TODO: get this vocab_size from target.pt?\n",
    "#         k = best_idx // vocab_size\n",
    "#         best_token_idx = best_idx % vocab_size\n",
    "#         best_token = trg_data['field'].vocab.itos[best_token_idx]\n",
    "#         result.append(best_token)\n",
    "#     return ' '.join(result[::-1])\n",
    "\n",
    "\n",
    "# def main():\n",
    "#     parser = argparse.ArgumentParser()\n",
    "#     parser.add_argument('--data_dir', type=str, required=True)\n",
    "#     parser.add_argument('--model_dir', type=str, required=True)\n",
    "#     parser.add_argument('--max_length', type=int, default=100)\n",
    "#     parser.add_argument('--beam_size', type=int, default=4)\n",
    "#     parser.add_argument('--alpha', type=float, default=0.6)\n",
    "#     parser.add_argument('--no_cuda', action='store_true')\n",
    "#     parser.add_argument('--translate', action='store_true')\n",
    "#     args = parser.parse_args()\n",
    "\n",
    "#     beam_size = args.beam_size\n",
    "\n",
    "#     # Load fields.\n",
    "#     if args.translate:\n",
    "#         src_data = torch.load(args.data_dir + '/source.pt')\n",
    "#     trg_data = torch.load(args.data_dir + '/target.pt')\n",
    "\n",
    "#     # Load a saved model.\n",
    "#     device = torch.device('cpu' if args.no_cuda else 'cuda')\n",
    "#     model = utils.load_checkpoint(args.model_dir, device)\n",
    "\n",
    "#     pads = torch.tensor([trg_data['pad_idx']] * beam_size, device=device)\n",
    "#     pads = pads.unsqueeze(-1)\n",
    "\n",
    "#     # We'll find a target sequence by beam search.\n",
    "#     scores_history = [torch.zeros((beam_size,), dtype=torch.float,\n",
    "#                                   device=device)]\n",
    "#     indices_history = []\n",
    "#     cache = {}\n",
    "\n",
    "#     eos_idx = trg_data['field'].vocab.stoi[trg_data['field'].eos_token]\n",
    "\n",
    "#     if args.translate:\n",
    "#         sentence = input('Source? ')\n",
    "\n",
    "#     # Encoding inputs.\n",
    "#     if args.translate:\n",
    "#         start_time = time.time()\n",
    "#         enc_output, src_mask = encode_inputs(sentence, model, src_data,\n",
    "#                                              beam_size, device)\n",
    "#         targets = pads\n",
    "#         start_idx = 0\n",
    "#     else:\n",
    "#         enc_output, src_mask = None, None\n",
    "#         sentence = input('Target? ').split()\n",
    "#         for idx, _ in enumerate(sentence):\n",
    "#             sentence[idx] = trg_data['field'].vocab.stoi[sentence[idx]]\n",
    "#         sentence.append(trg_data['pad_idx'])\n",
    "#         targets = torch.tensor([sentence], device=device)\n",
    "#         start_idx = targets.size(1) - 1\n",
    "#         start_time = time.time()\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for idx in range(start_idx, args.max_length):\n",
    "#             if idx > start_idx:\n",
    "#                 targets = torch.cat((targets, pads), dim=1)\n",
    "#             t_self_mask = utils.create_trg_self_mask(targets.size()[1],\n",
    "#                                                      device=targets.device)\n",
    "\n",
    "#             t_mask = utils.create_pad_mask(targets, trg_data['pad_idx'])\n",
    "#             pred = model.decode(targets, enc_output, src_mask,\n",
    "#                                 t_self_mask, t_mask, cache)\n",
    "#             pred = pred[:, idx].squeeze(1)\n",
    "#             vocab_size = pred.size(1)\n",
    "\n",
    "#             pred = F.log_softmax(pred, dim=1)\n",
    "#             if idx == start_idx:\n",
    "#                 scores = pred[0]\n",
    "#             else:\n",
    "#                 scores = scores_history[-1].unsqueeze(1) + pred\n",
    "#             length_penalty = pow(((5. + idx + 1.) / 6.), args.alpha)\n",
    "#             scores = scores / length_penalty\n",
    "#             scores = scores.view(-1)\n",
    "\n",
    "#             best_scores, best_indices = scores.topk(beam_size, 0)\n",
    "#             scores_history.append(best_scores)\n",
    "#             indices_history.append(best_indices)\n",
    "\n",
    "#             # Stop searching when the best output of beam is EOS.\n",
    "#             if best_indices[0].item() % vocab_size == eos_idx:\n",
    "#                 break\n",
    "\n",
    "#             targets = update_targets(targets, best_indices, idx, vocab_size)\n",
    "\n",
    "#     result = get_result_sentence(indices_history, trg_data, vocab_size)\n",
    "#     print(\"Result: {}\".format(result))\n",
    "\n",
    "#     print(\"Elapsed Time: {:.2f} sec\".format(time.time() - start_time))\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
